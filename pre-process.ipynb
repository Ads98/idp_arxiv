{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Iterable, Tuple\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef276f54",
   "metadata": {},
   "source": [
    "### Suamry annoations of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a386c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_arxiv_from_jsonl.py\n",
    "import gzip, time, re, csv, sys\n",
    "from urllib.request import Request, urlopen, URLError, HTTPError\n",
    "\n",
    "# ---- CONFIG (edit these) ----\n",
    "JSONL_PATH = Path(r\"C:\\Users\\Adam\\Documents\\idp\\arxiv\\arxiv-metadata-oai-snapshot.json\")  # or .json.gz\n",
    "OUT_DIR  = Path(\"C:\\\\Users\\\\Adam\\\\Documents\\\\idp\\\\arxiv\\\\\")\n",
    "MANIFEST   = Path(r\"C:\\Users\\Adam\\Documents\\idp\\arxiv\\manifest.csv\")\n",
    "\n",
    "MAX_PAPERS     = 1000     # set None for all (huge)\n",
    "SUBJECT_FILTER = None     # e.g., \"cs.CL\", or \"cs.\" to capture a family, or None\n",
    "RATE_SECONDS   = 0.6      # be polite to arXiv\n",
    "RETRIES        = 3        # per file\n",
    "TIMEOUT        = 60       # seconds\n",
    "\n",
    "# -----------------------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def safe_filename(s: str) -> str:\n",
    "    return re.sub(r'[^a-zA-Z0-9._-]+', '_', s)\n",
    "\n",
    "def pdf_url_from_id(arxiv_id: str) -> str:\n",
    "    # Works for both old-style (e.g., \"supr-con/9609004\") and modern (\"2401.01234\")\n",
    "    arxiv_id = arxiv_id.strip()\n",
    "    return f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "\n",
    "def primary_category(categories: str) -> str:\n",
    "    # Take first whitespace-separated token as the \"primary\"\n",
    "    return (categories or \"\").split()[0] if categories else \"\"\n",
    "\n",
    "def open_lines(path: Path):\n",
    "    if path.suffix.lower() == \".gz\":\n",
    "        return gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "downloaded = 0\n",
    "skipped = 0\n",
    "errors = 0\n",
    "\n",
    "# prepare manifest writer\n",
    "write_header = not MANIFEST.exists()\n",
    "mf = open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "w = csv.DictWriter(mf, fieldnames=[\n",
    "    \"filename\",\"arxiv_id\",\"pdf_url\",\"primary_category\",\"categories\",\"status\"\n",
    "])\n",
    "if write_header:\n",
    "    w.writeheader()\n",
    "\n",
    "with open_lines(JSONL_PATH) as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        try:\n",
    "            row = json.loads(line)\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "        arx_id = (row.get(\"id\") or row.get(\"paper_id\") or \"\").strip()\n",
    "        cats   = row.get(\"categories\",\"\")\n",
    "        if not arx_id:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if SUBJECT_FILTER and SUBJECT_FILTER not in cats:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        url = pdf_url_from_id(arx_id)\n",
    "        out_file = OUT_DIR / f\"{safe_filename(arx_id)}.pdf\"\n",
    "        if out_file.exists() and out_file.stat().st_size > 0:\n",
    "            w.writerow({\n",
    "                \"filename\": out_file.name,\n",
    "                \"arxiv_id\": arx_id,\n",
    "                \"pdf_url\": url,\n",
    "                \"primary_category\": primary_category(cats),\n",
    "                \"categories\": cats,\n",
    "                \"status\": \"exists\"\n",
    "            })\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # download with simple retries\n",
    "        ok = False\n",
    "        for attempt in range(1, RETRIES+1):\n",
    "            try:\n",
    "                req = Request(url, headers={\"User-Agent\": \"arxiv-downloader/1.0\"})\n",
    "                with urlopen(req, timeout=TIMEOUT) as resp, open(out_file, \"wb\") as wout:\n",
    "                    wout.write(resp.read())\n",
    "                ok = True\n",
    "                break\n",
    "            except (HTTPError, URLError, TimeoutError) as e:\n",
    "                if attempt == RETRIES:\n",
    "                    sys.stderr.write(f\"ERROR {arx_id}: {e}\\n\")\n",
    "                time.sleep(1.0 * attempt)  # backoff\n",
    "\n",
    "        if ok:\n",
    "            w.writerow({\n",
    "                \"filename\": out_file.name,\n",
    "                \"arxiv_id\": arx_id,\n",
    "                \"pdf_url\": url,\n",
    "                \"primary_category\": primary_category(cats),\n",
    "                \"categories\": cats,\n",
    "                \"status\": \"downloaded\"\n",
    "            })\n",
    "            downloaded += 1\n",
    "        else:\n",
    "            w.writerow({\n",
    "                \"filename\": out_file.name,\n",
    "                \"arxiv_id\": arx_id,\n",
    "                \"pdf_url\": url,\n",
    "                \"primary_category\": primary_category(cats),\n",
    "                \"categories\": cats,\n",
    "                \"status\": \"error\"\n",
    "            })\n",
    "            errors += 1\n",
    "\n",
    "        if MAX_PAPERS and downloaded >= MAX_PAPERS:\n",
    "            break\n",
    "\n",
    "        time.sleep(RATE_SECONDS)\n",
    "\n",
    "mf.close()\n",
    "print(f\"done: downloaded={downloaded} skipped={skipped} errors={errors} â†’ {OUT_DIR}\")\n",
    "print(f\"manifest: {MANIFEST}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92bac6f",
   "metadata": {},
   "source": [
    "### Randomly merge docuemnts to test seperation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aaa023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079077b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
