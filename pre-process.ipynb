{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Iterable, Tuple\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef276f54",
   "metadata": {},
   "source": [
    "### Suamry annoations of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a386c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_pdfs_to_categories.py\n",
    "import json, gzip, csv, re\n",
    "from pathlib import Path\n",
    "\n",
    "# --- EDIT THESE ---\n",
    "PDF_ROOT   = Path(r\"C:\\Users\\Adam\\Documents\\idp\\arxiv_pdf\")  # e.g., \"/Users/you/Downloads/arxiv/pdf\"\n",
    "META_PATH  = Path(r\"C:\\Users\\Adam\\Documents\\idp\\arxiv\\arxiv-metadata-oai-snapshot.json\")  # or .json\n",
    "MANIFEST   = Path(r\"C:\\Users\\Adam\\Documents\\idp\\arxiv\\manifest.csv\")\n",
    "# -------------------\n",
    "\n",
    "def open_lines(p: Path):\n",
    "    p = Path(p)\n",
    "    if str(p).lower().endswith(\".gz\"):\n",
    "        return gzip.open(p, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return open(p, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def primary_category(categories: str) -> str:\n",
    "    cats = (categories or \"\").split()\n",
    "    # pick first \"modern\" cat with a dot; else fall back to first token\n",
    "    for c in cats:\n",
    "        if \".\" in c:\n",
    "            return c\n",
    "    return cats[0] if cats else \"\"\n",
    "\n",
    "def load_id_to_cats(meta_path: Path):\n",
    "    id2cats = {}\n",
    "    with open_lines(meta_path) as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            row = json.loads(line)\n",
    "            arx_id = (row.get(\"id\") or row.get(\"paper_id\") or \"\").strip()\n",
    "            if not arx_id:\n",
    "                continue\n",
    "            id2cats[arx_id] = row.get(\"categories\",\"\")\n",
    "    return id2cats\n",
    "\n",
    "def arxiv_id_from_path(pdf_path: Path, pdf_root: Path) -> str:\n",
    "    \"\"\"\n",
    "    Reconstruct arXiv ID from the local PDF path under pdf_root.\n",
    "\n",
    "    Handles:\n",
    "      - modern layout: pdf/2401/2401.01234.pdf -> '2401.01234'\n",
    "      - legacy layout: pdf/cond-mat/9609001.pdf -> 'cond-mat/9609001'\n",
    "      - legacy w/ deeper trees (rare): join subdirs (except extension)\n",
    "    \"\"\"\n",
    "    rel = pdf_path.relative_to(pdf_root)\n",
    "    parts = rel.parts\n",
    "    stem = pdf_path.stem  # filename without .pdf\n",
    "\n",
    "    stem = re.sub(r'v\\d+$', '', stem)\n",
    "\n",
    "    # modern IDs: folder like '2401/2401.01234.pdf' (first part is 4-digit year+month)\n",
    "    if len(parts) >= 2 and len(parts[0]) == 4 and parts[0].isdigit():\n",
    "        return stem  # '2401.01234'\n",
    "\n",
    "    # legacy IDs: 'archive/number.pdf' -> 'archive/number'\n",
    "    if len(parts) >= 2:\n",
    "        # rebuild 'archive/number' (or deeper legacy variants) without .pdf\n",
    "        # e.g., 'cond-mat/9609001' or 'hep-th/9901001'\n",
    "        without_ext = Path(*parts).as_posix()[:-4]  # strip .pdf\n",
    "        return without_ext\n",
    "\n",
    "    # fallback: single file under root -> assume modern id equals stem\n",
    "    return stem\n",
    "\n",
    "def build_manifest(pdf_root: Path, id2cats: dict, out_csv: Path):\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    found = missing = 0\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\n",
    "            \"pdf_path\",\"arxiv_id\",\"primary_category\",\"categories\",\"matched_metadata\"\n",
    "        ])\n",
    "        w.writeheader()\n",
    "\n",
    "        for pdf in pdf_root.rglob(\"*.pdf\"):\n",
    "            arx_id = arxiv_id_from_path(pdf, pdf_root)\n",
    "            cats   = id2cats.get(arx_id, \"\")\n",
    "            pcat   = primary_category(cats)\n",
    "            matched = \"yes\" if cats else \"no\"\n",
    "            if matched == \"yes\":\n",
    "                found += 1\n",
    "            else:\n",
    "                missing += 1\n",
    "            w.writerow({\n",
    "                \"pdf_path\": str(pdf),\n",
    "                \"arxiv_id\": arx_id,\n",
    "                \"primary_category\": pcat,\n",
    "                \"categories\": cats,\n",
    "                \"matched_metadata\": matched\n",
    "            })\n",
    "    return found, missing\n",
    "\n",
    "def main():\n",
    "    print(\"Loading metadata…\")\n",
    "    id2cats = load_id_to_cats(META_PATH)\n",
    "    print(f\"Metadata IDs loaded: {len(id2cats):,}\")\n",
    "\n",
    "    print(\"Linking local PDFs to metadata…\")\n",
    "    found, missing = build_manifest(PDF_ROOT, id2cats, MANIFEST)\n",
    "    print(f\"Done. matched={found:,}  missing={missing:,}\")\n",
    "    print(f\"Manifest → {MANIFEST}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick sanity checks\n",
    "    assert PDF_ROOT.exists(), f\"PDF_ROOT not found: {PDF_ROOT}\"\n",
    "    assert META_PATH.exists(), f\"META_PATH not found: {META_PATH}\"\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92bac6f",
   "metadata": {},
   "source": [
    "### Randomly merge docuemnts to test seperation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aaa023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079077b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
